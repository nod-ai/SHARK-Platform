import torch
from sharktank.examples.paged_llm_v1 import *
from sharktank.utils import tokenizer
import unittest
from pathlib import Path

default_arguments = {
    "tokenizer-config-json": Path(
        "/home/avsharma/Llama-2-7b-chat-hf/tokenizer_config.json"
    ),
    "prompt": ["What is MLIR?"],
    "kv-cache-type": "direct",
    "device": None,
    "activation-dtype": "float32",
}

llama_cpp_7b_results = [
    [
        13,
        13,
        1988,
        8193,
        313,
        1988,
        4124,
        13847,
        16314,
        362,
        29897,
        338,
        263,
        716,
        29892,
        1722,
        29899,
        4993,
        19697,
        8954,
        313,
        8193,
        29897,
        363,
        4933,
        6509,
        313,
        1988,
        29897,
        4733,
        29889,
        739,
        338,
        8688,
        304,
        3867,
        263,
        3619,
        29892,
        7481,
        29899,
        21780,
        3402,
        363,
        23158,
        4733,
        393,
        508,
        367,
        1304,
        4822,
        1422,
        12837,
        322,
        7047,
        21796,
        29892,
        3704,
        10808,
        29879,
        29892,
        22796,
        29879,
        29892,
        323,
        7056,
        29879,
        29892,
        322,
        383,
        16903,
        2887,
        29889,
        13,
        13,
        1576,
        7306,
        310,
        341,
        5265,
        29934,
        338,
        304,
        3867,
        263,
        901,
        8543,
        322,
        25706,
        982,
        310,
        7246,
        292,
        23158,
        4733,
        29892,
        491,
        14372,
        963,
        304,
        367,
        27545,
        322,
        8283,
        373,
        263,
        9377,
        3464,
        310,
        12837,
        322,
        7047,
        21796,
        29889,
        910,
        508,
        1371,
        304,
        11157,
        278,
        4180,
        322,
        19201,
        310,
        23158,
        4733,
        29892,
        322,
        1207,
        963,
        901,
        15579,
        304,
        263,
        25734,
        3464,
        310,
        4160,
        29889,
        13,
        13,
        1988,
        8193,
        338,
        1641,
        8906,
        491,
        263,
        24771,
        310,
        13661,
        20251,
        29892,
        3704,
        5087,
        29892,
        405,
        13044,
        10764,
        29892,
        322,
        27955,
        29892,
        322,
        338,
        3806,
        304,
        367,
        17644,
        16356,
        4822,
        278,
        23158,
        7881,
        29889,
        739,
        338,
        2307,
        1641,
        1304,
        297,
        263,
        1353,
        310,
        5802,
        23136,
        29892,
        3704,
        5087,
        29915,
        29879,
        323,
        6073,
        17907,
        322,
        405,
        13044,
        10764,
        29915,
        29879,
        323,
        6073,
        13079,
        29889,
        13,
        13,
        2558,
        5680,
        310,
        341,
        5265,
        29934,
        3160,
        29901,
        13,
        13,
        29930,
        28096,
        29899,
        21780,
        29901,
        341,
        5265,
        29934,
        338,
        8688,
        304,
        367,
        1304,
        4822,
        263,
        9377,
        3464,
        310,
        12837,
        322,
        7047,
        21796,
        29892,
        3704,
        10808,
        29879,
        29892,
        22796,
        29879,
        29892,
        323,
        7056,
        29879,
        29892,
        322,
        383,
        16903,
        2887,
        29889,
        13,
        29930,
        4124,
        13847,
        8954,
        29901,
        341,
        5265,
        29934,
        338,
        385,
        19697,
        8954,
        310,
        23158,
        4733,
        29892,
        3265,
        1135,
        263,
        2186,
        29892,
        16813,
        3402,
        29889,
        910,
        6511,
        372,
        304,
        367,
        901,
        5948,
        27545,
        322,
        27615,
        363,
        1422,
        12837,
        322,
        7047,
        21796,
        29889,
        13,
        29930,
        7338,
        575,
        4127,
        29901,
        341,
        5265,
        29934,
        338,
        8688,
        304,
        367,
        21103,
        1821,
        29892,
        14372,
        716,
        5680,
        322,
        9863,
        304,
        367,
        2715,
        408,
        4312,
        29889,
        13,
        29930,
        3831,
        271,
        4127,
        29901,
        341,
        5265,
        29934,
        338,
        8688,
        304,
        367,
        15878,
        411,
        263,
        9377,
        3464,
        310,
        23158,
        29143,
        322,
        9562,
        29892,
        3704,
        323,
        6073,
        17907,
        29892,
        10772,
        29911,
        25350,
        29892,
        322,
        315,
        3470,
        29872,
        29889,
        13,
        13,
        3563,
        497,
        29892,
        341,
        5265,
        29934,
        338,
        385,
        4100,
        5849,
        297,
        278,
        1746,
        310,
        23158,
        29892,
        408,
        372,
        756,
        278,
        7037,
        304,
        11157,
        278,
        4180,
        322,
        19201,
        310,
        23158,
        4733,
        29892,
        322,
        1207,
        963,
        901,
        15579,
        304,
        263,
        25734,
        3464,
        310,
        4160,
        29889,
    ]
]


class Llama2Test(unittest.TestCase):
    def test7BModel(self):
        device = (
            torch.device(default_arguments["device"])
            if default_arguments["device"]
            else None
        )
        activation_dtype = getattr(torch, default_arguments["activation-dtype"])
        assert isinstance(activation_dtype, torch.dtype)
        dataset = Dataset.load("../Llama-2-7B-f16.gguf", file_type="gguf")
        tokenizer_config = tokenizer.load_tokenizer(
            default_arguments["tokenizer-config-json"].parent,
            tokenizer_type="transformers",
        )
        prompts = default_arguments["prompt"]
        config = LlamaModelConfig(
            hp=configs.LlamaHParams.from_gguf_props(dataset.properties),
            block_seq_stride=1,
            kv_cache_type=default_arguments["kv-cache-type"],
            device=device,
            activation_dtype=activation_dtype,
            attention_dtype=activation_dtype,
        )
        model = PagedLlamaModelV1(dataset.root_theta, config)
        generator = TorchGenerator(model, tokenizer_config)

        batch = generator.begin_batch(prompts)
        batch.prefill()

        while not batch.done:
            batch.decode()

        assert batch.results == llama_cpp_7b_results
