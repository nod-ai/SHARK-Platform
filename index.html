<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">index.html</title>
      <link href="assets/style.css" rel="stylesheet" type="text/css"/>
  </head>
  <body>
    <h1 id="title">index.html</h1>
    <p>Report generated on 15-Nov-2024 at 23:03:26 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="5">No results found. Check the filters.</th>
        </tr>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="5">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left"><</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">></div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">12 tests took 03:15:57.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" disabled/>
            <span class="failed">0 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" />
            <span class="passed">1 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" disabled/>
            <span class="skipped">0 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" />
            <span class="xfailed">11 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled/>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" disabled/>
            <span class="error">0 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" disabled/>
            <span class="rerun">0 Reruns</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th>XFail Reason</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  </body>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.11.10&#34;, &#34;Platform&#34;: &#34;Linux-6.8.0-47-generic-x86_64-with-glibc2.35&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;8.0.0&#34;, &#34;pluggy&#34;: &#34;1.5.0&#34;}, &#34;Plugins&#34;: {&#34;anyio&#34;: &#34;4.6.2.post1&#34;, &#34;html&#34;: &#34;4.1.1&#34;, &#34;xdist&#34;: &#34;3.5.0&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;}, &#34;CI&#34;: &#34;true&#34;}, &#34;tests&#34;: {&#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Decomposed&#34;, &#34;duration&#34;: &#34;00:30:39&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:30:39&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/fp16/llama3.1_8b_fp16.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_decomposed.json --bs=4 --attention-kernel decomposed\nINFO     eval:export_artifacts.py:186 Exported to mlir successfully:\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_decomposed.mlir&amp;#x27;\n\nINFO     eval:export_artifacts.py:117  export_to_mlir: 0:01:13\nINFO     eval:export_artifacts.py:117  compile_to_vmfb: 0:28:56\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Non_Decomposed&#34;, &#34;duration&#34;: &#34;00:29:26&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Compile Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:29:26&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_f16_Non_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(reason=&amp;quot;Compile Error&amp;quot;, strict=True, raises=IreeCompileException)\n    def testBenchmark8B_f16_Non_Decomposed(self):\n        output_file_name = self.dir_path_8b / &amp;quot;f16_torch&amp;quot;\n        output_mlir = self.llama8b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama8b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama8b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        self.llama8b_f16_torch_sdpa_artifacts.attention_kernel = &amp;quot;torch&amp;quot;\n        export_return_code = self.llama8b_f16_torch_sdpa_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n&amp;gt;       self.llama8b_f16_torch_sdpa_artifacts.compile_to_vmfb(\n            mlir_path=str(output_mlir),\n            vmfb_path=output_vmfb,\n            hal_dump_path=output_file_name,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:193: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a5fb5d0&amp;gt;\n\n    @timeit\n    def compile_to_vmfb(\n        self,\n        *,\n        mlir_path,\n        vmfb_path,\n        cwd,\n        hal_dump_path: Optional[Path] = None,\n    ):\n        # TODO: Control flag to enable multiple backends\n        compile_args = [\n            f&amp;quot;iree-compile&amp;quot;,\n            f&amp;quot;{mlir_path}&amp;quot;,\n            f&amp;quot;--iree-hip-target={self.iree_hip_target}&amp;quot;,\n            f&amp;quot;--iree-hal-target-backends={self.iree_hal_target_backends}&amp;quot;,\n            f&amp;quot;-o={vmfb_path}&amp;quot;,\n        ]\n        if self.tensor_parallelism_size &amp;gt; 1:\n            iree_hal_target_devices = [\n                f&amp;quot;--iree-hal-target-device=hip[{i}]&amp;quot;\n                for i in range(self.tensor_parallelism_size)\n            ]\n            compile_args += iree_hal_target_devices\n        if hal_dump_path:\n            compile_args += [\n                f&amp;quot;--iree-hal-dump-executable-files-to={hal_dump_path}/files&amp;quot;\n            ]\n    \n        cmd = subprocess.list2cmdline(compile_args)\n    \n        logging.getLogger().info(f&amp;quot;Launching compile command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise IreeCompileException(proc, cwd)\nE           sharktank.utils.export_artifacts.IreeCompileException: Error invoking iree-compile\nE           Error code: 1\nE           Stderr diagnostics:\nE           failed to translate executables\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch/files/configured_module_decode_bs4$async_dispatch_29.mlir:2:2: error: failed to run translation of source executable to target executable for backend #hal.executable.target&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;\nE             hal.executable.variant public @rocm_hsaco_fb target(&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;) {\nE            ^\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch/files/configured_module_decode_bs4$async_dispatch_29.mlir:2:2: note: see current operation: \nE           &amp;quot;hal.executable.variant&amp;quot;() ({\nE             &amp;quot;hal.executable.export&amp;quot;() ({\nE             ^bb0(%arg1: !hal.device, %arg2: index, %arg3: index, %arg4: index):\nE               %37 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 4 : index}&amp;gt; : () -&amp;gt; index\nE               %38 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 8 : index}&amp;gt; : () -&amp;gt; index\nE               &amp;quot;hal.return&amp;quot;(%37, %38, %37) : (index, index, index) -&amp;gt; ()\nE             }) {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 0 : index, subgroup_size = 64 : index, sym_name = &amp;quot;decode_bs4$async_dispatch_29_attention_4xDx16x8x4x128xf16_dispatch_tensor_store&amp;quot;, workgroup_size = [128 : index, 1 : index, 1 : index]} : () -&amp;gt; ()\nE             &amp;quot;builtin.module&amp;quot;() ({\nE               &amp;quot;func.func&amp;quot;() &amp;lt;{function_type = () -&amp;gt; (), sym_name = &amp;quot;decode_bs4$async_dispatch_29_attention_4xDx16x8x4x128xf16_dispatch_tensor_store&amp;quot;}&amp;gt; ({\nE                 %0 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 0 : index}&amp;gt; : () -&amp;gt; index\nE                 %1 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 32 : i64}&amp;gt; : () -&amp;gt; i64\nE                 %2 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 8.837890e-02 : f16}&amp;gt; : () -&amp;gt; f16\nE                 %3 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 0 : index} : () -&amp;gt; i32\nE                 %4 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 1 : index} : () -&amp;gt; i32\nE                 %5 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 2 : index} : () -&amp;gt; i32\nE                 %6 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 3 : index} : () -&amp;gt; i32\nE                 %7 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 4 : index} : () -&amp;gt; i32\nE                 %8 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 5 : index} : () -&amp;gt; i32\nE                 %9 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 6 : index} : () -&amp;gt; i32\nE                 %10 = &amp;quot;arith.index_castui&amp;quot;(%3) : (i32) -&amp;gt; index\nE                 %11 = &amp;quot;arith.extui&amp;quot;(%4) : (i32) -&amp;gt; i64\nE                 %12 = &amp;quot;arith.extui&amp;quot;(%5) : (i32) -&amp;gt; i64\nE                 %13 = &amp;quot;arith.shli&amp;quot;(%12, %1) &amp;lt;{overflowFlags = #arith.overflow&amp;lt;none&amp;gt;}&amp;gt; : (i64, i64) -&amp;gt; i64\nE                 %14 = &amp;quot;arith.ori&amp;quot;(%11, %13) : (i64, i64) -&amp;gt; i64\nE                 %15 = &amp;quot;arith.index_castui&amp;quot;(%14) : (i64) -&amp;gt; index\nE                 %16 = &amp;quot;arith.extui&amp;quot;(%6) : (i32) -&amp;gt; i64\nE                 %17 = &amp;quot;arith.extui&amp;quot;(%7) : (i32) -&amp;gt; i64\nE                 %18 = &amp;quot;arith.shli&amp;quot;(%17, %1) &amp;lt;{overflowFlags = #arith.overflow&amp;lt;none&amp;gt;}&amp;gt; : (i64, i64) -&amp;gt; i64\nE                 %19 = &amp;quot;arith.ori&amp;quot;(%16, %18) : (i64, i64) -&amp;gt; i64\nE                 %20 = &amp;quot;arith.index_castui&amp;quot;(%19) : (i64) -&amp;gt; index\nE                 %21 = &amp;quot;arith.index_castui&amp;quot;(%8) : (i32) -&amp;gt; index\nE                 %22 = &amp;quot;arith.index_castui&amp;quot;(%9) : (i32) -&amp;gt; index\nE                 %23:5 = &amp;quot;util.assume.int&amp;quot;(%10, %15, %20, %21, %22) &amp;lt;{assumptions = [[#util.int.assumption&amp;lt;umin = 67406016, umax = 2215151616&amp;gt;], [#util.int.assumption&amp;lt;umin = 67930304, umax = 6509594624&amp;gt;], [#util.int.assumption&amp;lt;umin = 68454592, umax = 10804037632&amp;gt;], [#util.int.assumption&amp;lt;umin = 49600, umax = 49600, udiv = 49600&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;], [#util.int.assumption&amp;lt;umin = 1, umax = 8191&amp;gt;]]}&amp;gt; : (index, index, index, index, index) -&amp;gt; (index, index, index, index, index)\nE                 %24 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%0) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 0&amp;gt;} : (index) -&amp;gt; memref&amp;lt;4x8x4x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%24) &amp;lt;{alignment = 64 : i32}&amp;gt; : (memref&amp;lt;4x8x4x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %25 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#3) {alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 0&amp;gt;} : (index) -&amp;gt; memref&amp;lt;4x8x4x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%25) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x8x4x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %26 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 2 : index} : () -&amp;gt; index\nE                 %27 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 1 : index} : () -&amp;gt; index\nE                 %28 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 0 : index} : () -&amp;gt; index\nE                 %29 = &amp;quot;memref.subview&amp;quot;(%25, %26, %27, %28) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 0, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x4x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %30 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#0, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x?x16x8x4x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%30) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x?x16x8x4x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %31 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#1, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x?x16x8x4x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%31) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x?x16x8x4x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %32 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#2, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x8x4x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%32) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x8x4x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %33 = &amp;quot;memref.subview&amp;quot;(%24, %26, %27, %28) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 0, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x4x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %34 = &amp;quot;memref.subview&amp;quot;(%30, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, 0, 0, -9223372036854775808, -9223372036854775808, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, -9223372036854775808, 16, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x?x16x8x4x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %35 = &amp;quot;memref.subview&amp;quot;(%31, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, 0, 0, -9223372036854775808, -9223372036854775808, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, -9223372036854775808, 16, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x?x16x8x4x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %36 = &amp;quot;memref.subview&amp;quot;(%32, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, -9223372036854775808, 16&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x4x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;iree_linalg_ext.attention&amp;quot;(%33, %34, %35, %2, %36, %29) &amp;lt;{indexing_maps = [affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d5)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d6, d7, d1, d2, d5)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d6, d7, d1, d2, d4)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; ()&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d6, d7)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d4)&amp;gt;]}&amp;gt; ({\nE                 ^bb0(%arg0: f32):\nE                   &amp;quot;iree_linalg_ext.yield&amp;quot;(%arg0) : (f32) -&amp;gt; ()\nE                 }) : (memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 65536, 4096, 512, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, f16, memref&amp;lt;1x1x1x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[4096, 512, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 &amp;quot;func.return&amp;quot;() : () -&amp;gt; ()\nE               }) : () -&amp;gt; ()\nE             }) : () -&amp;gt; ()\nE             &amp;quot;hal.executable.variant_end&amp;quot;() : () -&amp;gt; ()\nE           }) {sym_name = &amp;quot;rocm_hsaco_fb&amp;quot;, target = #hal.executable.target&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;} : () -&amp;gt; ()\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; iree-compile /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch.mlir --iree-hip-target=gfx942 --iree-hal-target-backends=rocm -o=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch.vmfb --iree-hal-dump-executable-files-to=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch/files\n\nsharktank/sharktank/utils/export_artifacts.py:224: IreeCompileException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/fp16/llama3.1_8b_fp16.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch.json --bs=4 --attention-kernel torch\nINFO     eval:export_artifacts.py:186 Exported to mlir successfully:\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/f16_torch.mlir&amp;#x27;\n\nINFO     eval:export_artifacts.py:117  export_to_mlir: 0:01:01\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Test not yet implemented&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_fp8_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Test not yet implemented&amp;quot;, strict=True, raises=ExportMlirException\n    )\n    def testBenchmark8B_fp8_Decomposed(self):\n        output_file_name = self.dir_path_8b / &amp;quot;fp8_decomposed&amp;quot;\n        output_mlir = self.llama8b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama8b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama8b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n&amp;gt;       export_return_code = self.llama8b_fp8_decomposed_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:230: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a5d3e10&amp;gt;\n\n    @timeit\n    def export_to_mlir(\n        self,\n        *,\n        mlir_path: str,\n        json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            f&amp;quot;--irpa-file={self.irpa_path}&amp;quot;,\n            f&amp;quot;--output-mlir={mlir_path}&amp;quot;,\n            f&amp;quot;--output-config={json_path}&amp;quot;,\n            f&amp;quot;--bs={str(self.batch_size)}&amp;quot;,\n        ]\n        if self.attention_kernel in [&amp;quot;decomposed&amp;quot;, &amp;quot;torch&amp;quot;]:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(self.attention_kernel)\n    \n        cwd = self.sharktank_dir\n        cmd = subprocess.list2cmdline(export_args)\n    \n        logger.info(f&amp;quot;Exporting mlir:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n    \n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd, text=True)\n        if proc.returncode != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           sharktank.utils.export_artifacts.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 336, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 69, in main\nE               hp = configs.LlamaHParams.from_gguf_props(dataset.properties)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/configs/llm_configs.py&amp;quot;, line 47, in from_gguf_props\nE               name_prefix = p[&amp;quot;general.architecture&amp;quot;]\nE                             ~^^^^^^^^^^^^^^^^^^^^^^^^\nE           KeyError: &amp;#x27;general.architecture&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/f8/llama8b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_decomposed.json --bs=4 --attention-kernel decomposed\n\nsharktank/sharktank/utils/export_artifacts.py:184: ExportMlirException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/f8/llama8b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_decomposed.json --bs=4 --attention-kernel decomposed\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Test not yet implemented&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_fp8_Non_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Test not yet implemented&amp;quot;, strict=True, raises=ExportMlirException\n    )\n    def testBenchmark8B_fp8_Non_Decomposed(self):\n        output_file_name = self.dir_path_8b / &amp;quot;fp8_torch&amp;quot;\n        output_mlir = self.llama8b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama8b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama8b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n&amp;gt;       export_return_code = self.llama8b_fp8_torch_sdpa_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:271: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a5d3110&amp;gt;\n\n    @timeit\n    def export_to_mlir(\n        self,\n        *,\n        mlir_path: str,\n        json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            f&amp;quot;--irpa-file={self.irpa_path}&amp;quot;,\n            f&amp;quot;--output-mlir={mlir_path}&amp;quot;,\n            f&amp;quot;--output-config={json_path}&amp;quot;,\n            f&amp;quot;--bs={str(self.batch_size)}&amp;quot;,\n        ]\n        if self.attention_kernel in [&amp;quot;decomposed&amp;quot;, &amp;quot;torch&amp;quot;]:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(self.attention_kernel)\n    \n        cwd = self.sharktank_dir\n        cmd = subprocess.list2cmdline(export_args)\n    \n        logger.info(f&amp;quot;Exporting mlir:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n    \n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd, text=True)\n        if proc.returncode != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           sharktank.utils.export_artifacts.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 336, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 69, in main\nE               hp = configs.LlamaHParams.from_gguf_props(dataset.properties)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/configs/llm_configs.py&amp;quot;, line 47, in from_gguf_props\nE               name_prefix = p[&amp;quot;general.architecture&amp;quot;]\nE                             ~^^^^^^^^^^^^^^^^^^^^^^^^\nE           KeyError: &amp;#x27;general.architecture&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/f8/llama8b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_torch.json --bs=4 --attention-kernel torch\n\nsharktank/sharktank/utils/export_artifacts.py:184: ExportMlirException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/f8/llama8b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-8b/fp8_torch.json --bs=4 --attention-kernel torch\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_TP8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_TP8_Decomposed&#34;, &#34;duration&#34;: &#34;00:33:22&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_TP8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Benchmarking Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:33:22&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_f16_TP8_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Benchmarking Error&amp;quot;, strict=True, raises=IreeBenchmarkException\n    )\n    def testBenchmark70B_f16_TP8_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;f16_decomposed&amp;quot;\n        output_mlir = self.llama70b_f16_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama70b_f16_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama70b_f16_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;fp16/tp8/llama3.1_70b_fp16_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n        export_return_code = self.llama70b_f16_decomposed_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n        self.llama70b_f16_decomposed_artifacts.compile_to_vmfb(\n            mlir_path=str(output_mlir),\n            vmfb_path=output_vmfb,\n            hal_dump_path=output_file_name,\n            cwd=self.repo_root,\n        )\n        # benchmark prefill\n&amp;gt;       self.llama70b_f16_decomposed_artifacts.iree_benchmark_vmfb(\n            hip_device_id=self.hip_device_id,\n            vmfb_name=output_vmfb,\n            irpa_path=self.irpa_path,\n            args=self.iree_run_prefill_args,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:415: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a51bf90&amp;gt;\n\n    def iree_benchmark_vmfb(\n        self,\n        *,\n        hip_device_id: str,\n        vmfb_name: str,\n        irpa_path: str,\n        args: List[str],\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs a compiled program with the given args using `iree-benchmark-module`.\n        This assumes that the `iree-benchmark-module` command is available (usually via PATH).\n        Args:\n            vmfb_name: Name of the .vmfb file (relative to `cwd`).\n            args: List of arguments to pass to `iree-benchmark-module`.\n            cwd: Working directory to run the command within. (either string or Path works)\n            compile_cmd: Command used to compile the program, for inclusion in error messages.\n        Raises Exception if running fails for some reason.\n        &amp;quot;&amp;quot;&amp;quot;\n        benchmark_args = []\n        if self.tensor_parallelism_size &amp;gt; 1:\n            base_irpa_path, _ = os.path.splitext(irpa_path)\n            rocr_visible_devices = [\n                f&amp;quot;ROCR_VISIBLE_DEVICES={&amp;#x27;,&amp;#x27;.join(str(i) for i in range(self.tensor_parallelism_size))}&amp;quot;\n            ]\n            params = [f&amp;quot;--parameters=model={base_irpa_path}.irpa&amp;quot;]\n            params += [\n                f&amp;quot;--parameters=model={base_irpa_path}.rank{i}.irpa&amp;quot;\n                for i in range(self.tensor_parallelism_size)\n            ]\n            devices = [\n                f&amp;quot;--device=hip://{i}&amp;quot; for i in range(self.tensor_parallelism_size)\n            ]\n        else:\n            rocr_visible_devices = [f&amp;quot;ROCR_VISIBLE_DEVICES={hip_device_id}&amp;quot;]\n            params = [f&amp;quot;--parameters=model={irpa_path}&amp;quot;]\n            devices = [f&amp;quot;--device=hip://{hip_device_id}&amp;quot;]\n        benchmark_args += rocr_visible_devices\n        benchmark_args += [\n            &amp;quot;iree-benchmark-module&amp;quot;,\n            &amp;quot;--hip_use_streams=true&amp;quot;,\n            &amp;quot;--hip_allow_inline_execution=true&amp;quot;,\n            &amp;quot;--device_allocator=caching&amp;quot;,\n            f&amp;quot;--module={vmfb_name}&amp;quot;,\n        ]\n        benchmark_args += params\n        benchmark_args += devices\n        benchmark_args += args\n        cmd = subprocess.list2cmdline(benchmark_args)\n        logging.getLogger().info(f&amp;quot;Launching run command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, stdout=sys.stdout, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise IreeBenchmarkException(proc, cwd)\nE           sharktank.utils.export_artifacts.IreeBenchmarkException: Error invoking iree-benchmark-module\nE           Error code: 5\nE           Stderr diagnostics:\nE           None\nE           Stdout diagnostics:\nE           None\nE           Run with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; ROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 iree-benchmark-module --hip_use_streams=true --hip_allow_inline_execution=true --device_allocator=caching --module=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_decomposed.vmfb --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank0.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank1.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank2.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank3.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank4.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank5.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank6.irpa --parameters=model=/data/llama-3.1/weights/70b/fp16/tp8/llama3.1_70b_fp16_tp8_parameters.rank7.irpa --device=hip://0 --device=hip://1 --device=hip://2 --device=hip://3 --device=hip://4 --device=hip://5 --device=hip://6 --device=hip://7 --function=prefill_bs4 --input=@/data/llama-3.1/weights/70b/prefill_args/tokens.npy --input=@/data/llama-3.1/weights/70b/prefill_args/seq_lens.npy --input=@/data/llama-3.1/weights/70b/prefill_args/seq_block_ids.npy --input=@/data/llama-3.1/weights/70b/prefill_args/cache_state_f16.npy --benchmark_repetitions=3\n\nsharktank/sharktank/utils/export_artifacts.py:278: IreeBenchmarkException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/70b/fp16/llama3.1_70b_f16.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_decomposed.json --bs=4 --attention-kernel decomposed\nINFO     eval:export_artifacts.py:186 Exported to mlir successfully:\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_decomposed.mlir&amp;#x27;\n\nINFO     eval:export_artifacts.py:117  export_to_mlir: 0:03:16\nINFO     eval:export_artifacts.py:117  compile_to_vmfb: 0:29:30\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_TP8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_TP8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;00:31:41&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_TP8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Compile Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:31:41&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_f16_TP8_Non_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(reason=&amp;quot;Compile Error&amp;quot;, strict=True, raises=IreeCompileException)\n    def testBenchmark70B_f16_TP8_Non_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;f16_torch&amp;quot;\n        output_mlir = self.llama70b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama70b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama70b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;fp16/tp8/llama3.1_70b_fp16_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n        export_return_code = self.llama70b_f16_torch_sdpa_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n&amp;gt;       self.llama70b_f16_torch_sdpa_artifacts.compile_to_vmfb(\n            mlir_path=str(output_mlir),\n            vmfb_path=output_vmfb,\n            hal_dump_path=output_file_name,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:453: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a4b1d90&amp;gt;\n\n    @timeit\n    def compile_to_vmfb(\n        self,\n        *,\n        mlir_path,\n        vmfb_path,\n        cwd,\n        hal_dump_path: Optional[Path] = None,\n    ):\n        # TODO: Control flag to enable multiple backends\n        compile_args = [\n            f&amp;quot;iree-compile&amp;quot;,\n            f&amp;quot;{mlir_path}&amp;quot;,\n            f&amp;quot;--iree-hip-target={self.iree_hip_target}&amp;quot;,\n            f&amp;quot;--iree-hal-target-backends={self.iree_hal_target_backends}&amp;quot;,\n            f&amp;quot;-o={vmfb_path}&amp;quot;,\n        ]\n        if self.tensor_parallelism_size &amp;gt; 1:\n            iree_hal_target_devices = [\n                f&amp;quot;--iree-hal-target-device=hip[{i}]&amp;quot;\n                for i in range(self.tensor_parallelism_size)\n            ]\n            compile_args += iree_hal_target_devices\n        if hal_dump_path:\n            compile_args += [\n                f&amp;quot;--iree-hal-dump-executable-files-to={hal_dump_path}/files&amp;quot;\n            ]\n    \n        cmd = subprocess.list2cmdline(compile_args)\n    \n        logging.getLogger().info(f&amp;quot;Launching compile command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise IreeCompileException(proc, cwd)\nE           sharktank.utils.export_artifacts.IreeCompileException: Error invoking iree-compile\nE           Error code: 1\nE           Stderr diagnostics:\nE           failed to translate executables\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch/files/configured_module_decode_bs4$async_dispatch_29.mlir:2:2: error: failed to run translation of source executable to target executable for backend #hal.executable.target&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;\nE             hal.executable.variant public @rocm_hsaco_fb target(&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;) {\nE            ^\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch/files/configured_module_decode_bs4$async_dispatch_29.mlir:2:2: note: see current operation: \nE           &amp;quot;hal.executable.variant&amp;quot;() ({\nE             &amp;quot;hal.executable.export&amp;quot;() ({\nE             ^bb0(%arg1: !hal.device, %arg2: index, %arg3: index, %arg4: index):\nE               %37 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 8 : index}&amp;gt; : () -&amp;gt; index\nE               %38 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 4 : index}&amp;gt; : () -&amp;gt; index\nE               &amp;quot;hal.return&amp;quot;(%37, %37, %38) : (index, index, index) -&amp;gt; ()\nE             }) {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 0 : index, subgroup_size = 64 : index, sym_name = &amp;quot;decode_bs4$async_dispatch_29_attention_4xDx16x8x8x128xf16_dispatch_tensor_store&amp;quot;, workgroup_size = [128 : index, 1 : index, 1 : index]} : () -&amp;gt; ()\nE             &amp;quot;builtin.module&amp;quot;() ({\nE               &amp;quot;func.func&amp;quot;() &amp;lt;{function_type = () -&amp;gt; (), sym_name = &amp;quot;decode_bs4$async_dispatch_29_attention_4xDx16x8x8x128xf16_dispatch_tensor_store&amp;quot;}&amp;gt; ({\nE                 %0 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 0 : index}&amp;gt; : () -&amp;gt; index\nE                 %1 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 32 : i64}&amp;gt; : () -&amp;gt; i64\nE                 %2 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 8.837890e-02 : f16}&amp;gt; : () -&amp;gt; f16\nE                 %3 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 0 : index} : () -&amp;gt; i32\nE                 %4 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 1 : index} : () -&amp;gt; i32\nE                 %5 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 2 : index} : () -&amp;gt; i32\nE                 %6 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 3 : index} : () -&amp;gt; i32\nE                 %7 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 4 : index} : () -&amp;gt; i32\nE                 %8 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 5 : index} : () -&amp;gt; i32\nE                 %9 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 6 : index} : () -&amp;gt; i32\nE                 %10 = &amp;quot;arith.index_castui&amp;quot;(%3) : (i32) -&amp;gt; index\nE                 %11 = &amp;quot;arith.extui&amp;quot;(%4) : (i32) -&amp;gt; i64\nE                 %12 = &amp;quot;arith.extui&amp;quot;(%5) : (i32) -&amp;gt; i64\nE                 %13 = &amp;quot;arith.shli&amp;quot;(%12, %1) &amp;lt;{overflowFlags = #arith.overflow&amp;lt;none&amp;gt;}&amp;gt; : (i64, i64) -&amp;gt; i64\nE                 %14 = &amp;quot;arith.ori&amp;quot;(%11, %13) : (i64, i64) -&amp;gt; i64\nE                 %15 = &amp;quot;arith.index_castui&amp;quot;(%14) : (i64) -&amp;gt; index\nE                 %16 = &amp;quot;arith.extui&amp;quot;(%6) : (i32) -&amp;gt; i64\nE                 %17 = &amp;quot;arith.extui&amp;quot;(%7) : (i32) -&amp;gt; i64\nE                 %18 = &amp;quot;arith.shli&amp;quot;(%17, %1) &amp;lt;{overflowFlags = #arith.overflow&amp;lt;none&amp;gt;}&amp;gt; : (i64, i64) -&amp;gt; i64\nE                 %19 = &amp;quot;arith.ori&amp;quot;(%16, %18) : (i64, i64) -&amp;gt; i64\nE                 %20 = &amp;quot;arith.index_castui&amp;quot;(%19) : (i64) -&amp;gt; index\nE                 %21 = &amp;quot;arith.index_castui&amp;quot;(%8) : (i32) -&amp;gt; index\nE                 %22 = &amp;quot;arith.index_castui&amp;quot;(%9) : (i32) -&amp;gt; index\nE                 %23:5 = &amp;quot;util.assume.int&amp;quot;(%10, %15, %20, %21, %22) &amp;lt;{assumptions = [[#util.int.assumption&amp;lt;umin = 67438784, umax = 2215184384&amp;gt;], [#util.int.assumption&amp;lt;umin = 68487360, umax = 10804070400&amp;gt;], [#util.int.assumption&amp;lt;umin = 69535936, umax = 19392956416&amp;gt;], [#util.int.assumption&amp;lt;umin = 82368, umax = 82368, udiv = 82368&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;], [#util.int.assumption&amp;lt;umin = 1, umax = 8191&amp;gt;]]}&amp;gt; : (index, index, index, index, index) -&amp;gt; (index, index, index, index, index)\nE                 %24 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%0) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 0&amp;gt;} : (index) -&amp;gt; memref&amp;lt;4x8x8x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%24) &amp;lt;{alignment = 64 : i32}&amp;gt; : (memref&amp;lt;4x8x8x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %25 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#3) {alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 0&amp;gt;} : (index) -&amp;gt; memref&amp;lt;4x8x8x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%25) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x8x8x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %26 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 2 : index} : () -&amp;gt; index\nE                 %27 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 1 : index} : () -&amp;gt; index\nE                 %28 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 0 : index} : () -&amp;gt; index\nE                 %29 = &amp;quot;memref.subview&amp;quot;(%25, %26, %27, %28) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 0, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x8x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %30 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#0, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x?x16x8x8x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%30) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x?x16x8x8x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %31 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#1, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x?x16x8x8x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%31) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x?x16x8x8x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %32 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#2, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x8x8x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%32) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x8x8x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %33 = &amp;quot;memref.subview&amp;quot;(%24, %26, %27, %28) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 0, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x8x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %34 = &amp;quot;memref.subview&amp;quot;(%30, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, 0, 0, -9223372036854775808, -9223372036854775808, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, -9223372036854775808, 16, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x?x16x8x8x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %35 = &amp;quot;memref.subview&amp;quot;(%31, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, 0, 0, -9223372036854775808, -9223372036854775808, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, -9223372036854775808, 16, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x?x16x8x8x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %36 = &amp;quot;memref.subview&amp;quot;(%32, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, -9223372036854775808, 16&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x8x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;iree_linalg_ext.attention&amp;quot;(%33, %34, %35, %2, %36, %29) &amp;lt;{indexing_maps = [affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d5)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d6, d7, d1, d2, d5)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d6, d7, d1, d2, d4)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; ()&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d6, d7)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d4)&amp;gt;]}&amp;gt; ({\nE                 ^bb0(%arg0: f32):\nE                   &amp;quot;iree_linalg_ext.yield&amp;quot;(%arg0) : (f32) -&amp;gt; ()\nE                 }) : (memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 131072, 8192, 1024, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, f16, memref&amp;lt;1x1x1x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[8192, 1024, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 &amp;quot;func.return&amp;quot;() : () -&amp;gt; ()\nE               }) : () -&amp;gt; ()\nE             }) : () -&amp;gt; ()\nE             &amp;quot;hal.executable.variant_end&amp;quot;() : () -&amp;gt; ()\nE           }) {sym_name = &amp;quot;rocm_hsaco_fb&amp;quot;, target = #hal.executable.target&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;} : () -&amp;gt; ()\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; iree-compile /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch.mlir --iree-hip-target=gfx942 --iree-hal-target-backends=rocm -o=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch.vmfb --iree-hal-target-device=hip[0] --iree-hal-target-device=hip[1] --iree-hal-target-device=hip[2] --iree-hal-target-device=hip[3] --iree-hal-target-device=hip[4] --iree-hal-target-device=hip[5] --iree-hal-target-device=hip[6] --iree-hal-target-device=hip[7] --iree-hal-dump-executable-files-to=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch/files\n\nsharktank/sharktank/utils/export_artifacts.py:224: IreeCompileException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/70b/fp16/llama3.1_70b_f16.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch.json --bs=4 --attention-kernel torch\nINFO     eval:export_artifacts.py:186 Exported to mlir successfully:\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/f16_torch.mlir&amp;#x27;\n\nINFO     eval:export_artifacts.py:117  export_to_mlir: 0:02:37\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_TP8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_TP8_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_TP8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Test not yet implemented&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_fp8_TP8_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Test not yet implemented&amp;quot;, strict=True, raises=ExportMlirException\n    )\n    def testBenchmark70B_fp8_TP8_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;fp8_decomposed&amp;quot;\n        output_mlir = self.llama70b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama70b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama70b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;f8/tp8/llama3.1_70b_fp8_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n&amp;gt;       export_return_code = self.llama70b_fp8_decomposed_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:496: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a4b9890&amp;gt;\n\n    @timeit\n    def export_to_mlir(\n        self,\n        *,\n        mlir_path: str,\n        json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            f&amp;quot;--irpa-file={self.irpa_path}&amp;quot;,\n            f&amp;quot;--output-mlir={mlir_path}&amp;quot;,\n            f&amp;quot;--output-config={json_path}&amp;quot;,\n            f&amp;quot;--bs={str(self.batch_size)}&amp;quot;,\n        ]\n        if self.attention_kernel in [&amp;quot;decomposed&amp;quot;, &amp;quot;torch&amp;quot;]:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(self.attention_kernel)\n    \n        cwd = self.sharktank_dir\n        cmd = subprocess.list2cmdline(export_args)\n    \n        logger.info(f&amp;quot;Exporting mlir:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n    \n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd, text=True)\n        if proc.returncode != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           sharktank.utils.export_artifacts.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 336, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 69, in main\nE               hp = configs.LlamaHParams.from_gguf_props(dataset.properties)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/configs/llm_configs.py&amp;quot;, line 47, in from_gguf_props\nE               name_prefix = p[&amp;quot;general.architecture&amp;quot;]\nE                             ~^^^^^^^^^^^^^^^^^^^^^^^^\nE           KeyError: &amp;#x27;general.architecture&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/70b/f8/llama70b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_decomposed.json --bs=4 --attention-kernel decomposed\n\nsharktank/sharktank/utils/export_artifacts.py:184: ExportMlirException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/70b/f8/llama70b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_decomposed.json --bs=4 --attention-kernel decomposed\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_TP8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_TP8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_TP8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Test not yet implemented&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_fp8_TP8_Non_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Test not yet implemented&amp;quot;, strict=True, raises=ExportMlirException\n    )\n    def testBenchmark70B_fp8_TP8_Non_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;fp8_torch&amp;quot;\n        output_mlir = self.llama70b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama70b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama70b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;f8/tp8/llama3.1_70b_f8_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n&amp;gt;       export_return_code = self.llama70b_fp8_torch_sdpa_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:543: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a4b1350&amp;gt;\n\n    @timeit\n    def export_to_mlir(\n        self,\n        *,\n        mlir_path: str,\n        json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            f&amp;quot;--irpa-file={self.irpa_path}&amp;quot;,\n            f&amp;quot;--output-mlir={mlir_path}&amp;quot;,\n            f&amp;quot;--output-config={json_path}&amp;quot;,\n            f&amp;quot;--bs={str(self.batch_size)}&amp;quot;,\n        ]\n        if self.attention_kernel in [&amp;quot;decomposed&amp;quot;, &amp;quot;torch&amp;quot;]:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(self.attention_kernel)\n    \n        cwd = self.sharktank_dir\n        cmd = subprocess.list2cmdline(export_args)\n    \n        logger.info(f&amp;quot;Exporting mlir:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n    \n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd, text=True)\n        if proc.returncode != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           sharktank.utils.export_artifacts.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 336, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 69, in main\nE               hp = configs.LlamaHParams.from_gguf_props(dataset.properties)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/configs/llm_configs.py&amp;quot;, line 47, in from_gguf_props\nE               name_prefix = p[&amp;quot;general.architecture&amp;quot;]\nE                             ~^^^^^^^^^^^^^^^^^^^^^^^^\nE           KeyError: &amp;#x27;general.architecture&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/70b/f8/llama70b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_torch.json --bs=4 --attention-kernel torch\n\nsharktank/sharktank/utils/export_artifacts.py:184: ExportMlirException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/70b/f8/llama70b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-70b/fp8_torch.json --bs=4 --attention-kernel torch\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_TP8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_TP8_Decomposed&#34;, &#34;duration&#34;: &#34;00:37:27&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_TP8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Benchmarking Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:37:27&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_f16_TP8_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Benchmarking Error&amp;quot;, strict=True, raises=IreeBenchmarkException\n    )\n    def testBenchmark405B_f16_TP8_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;f16_decomposed&amp;quot;\n        output_mlir = self.llama405b_f16_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama405b_f16_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama405b_f16_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;fp16/tp8/llama3.1_405b_fp16_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n        export_return_code = self.llama405b_f16_decomposed_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n        self.llama405b_f16_decomposed_artifacts.compile_to_vmfb(\n            mlir_path=str(output_mlir),\n            vmfb_path=output_vmfb,\n            hal_dump_path=output_file_name,\n            cwd=self.repo_root,\n        )\n        # benchmark prefill\n&amp;gt;       self.llama405b_f16_decomposed_artifacts.iree_benchmark_vmfb(\n            hip_device_id=self.hip_device_id,\n            vmfb_name=output_vmfb,\n            irpa_path=self.irpa_path,\n            args=self.iree_run_prefill_args,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:687: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a454c10&amp;gt;\n\n    def iree_benchmark_vmfb(\n        self,\n        *,\n        hip_device_id: str,\n        vmfb_name: str,\n        irpa_path: str,\n        args: List[str],\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs a compiled program with the given args using `iree-benchmark-module`.\n        This assumes that the `iree-benchmark-module` command is available (usually via PATH).\n        Args:\n            vmfb_name: Name of the .vmfb file (relative to `cwd`).\n            args: List of arguments to pass to `iree-benchmark-module`.\n            cwd: Working directory to run the command within. (either string or Path works)\n            compile_cmd: Command used to compile the program, for inclusion in error messages.\n        Raises Exception if running fails for some reason.\n        &amp;quot;&amp;quot;&amp;quot;\n        benchmark_args = []\n        if self.tensor_parallelism_size &amp;gt; 1:\n            base_irpa_path, _ = os.path.splitext(irpa_path)\n            rocr_visible_devices = [\n                f&amp;quot;ROCR_VISIBLE_DEVICES={&amp;#x27;,&amp;#x27;.join(str(i) for i in range(self.tensor_parallelism_size))}&amp;quot;\n            ]\n            params = [f&amp;quot;--parameters=model={base_irpa_path}.irpa&amp;quot;]\n            params += [\n                f&amp;quot;--parameters=model={base_irpa_path}.rank{i}.irpa&amp;quot;\n                for i in range(self.tensor_parallelism_size)\n            ]\n            devices = [\n                f&amp;quot;--device=hip://{i}&amp;quot; for i in range(self.tensor_parallelism_size)\n            ]\n        else:\n            rocr_visible_devices = [f&amp;quot;ROCR_VISIBLE_DEVICES={hip_device_id}&amp;quot;]\n            params = [f&amp;quot;--parameters=model={irpa_path}&amp;quot;]\n            devices = [f&amp;quot;--device=hip://{hip_device_id}&amp;quot;]\n        benchmark_args += rocr_visible_devices\n        benchmark_args += [\n            &amp;quot;iree-benchmark-module&amp;quot;,\n            &amp;quot;--hip_use_streams=true&amp;quot;,\n            &amp;quot;--hip_allow_inline_execution=true&amp;quot;,\n            &amp;quot;--device_allocator=caching&amp;quot;,\n            f&amp;quot;--module={vmfb_name}&amp;quot;,\n        ]\n        benchmark_args += params\n        benchmark_args += devices\n        benchmark_args += args\n        cmd = subprocess.list2cmdline(benchmark_args)\n        logging.getLogger().info(f&amp;quot;Launching run command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, stdout=sys.stdout, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise IreeBenchmarkException(proc, cwd)\nE           sharktank.utils.export_artifacts.IreeBenchmarkException: Error invoking iree-benchmark-module\nE           Error code: 5\nE           Stderr diagnostics:\nE           None\nE           Stdout diagnostics:\nE           None\nE           Run with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; ROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 iree-benchmark-module --hip_use_streams=true --hip_allow_inline_execution=true --device_allocator=caching --module=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_decomposed.vmfb --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank0.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank1.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank2.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank3.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank4.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank5.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank6.irpa --parameters=model=/data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank7.irpa --device=hip://0 --device=hip://1 --device=hip://2 --device=hip://3 --device=hip://4 --device=hip://5 --device=hip://6 --device=hip://7 --function=prefill_bs4 --input=@/data/llama-3.1/weights/405b/prefill_args/tokens.npy --input=@/data/llama-3.1/weights/405b/prefill_args/seq_lens.npy --input=@/data/llama-3.1/weights/405b/prefill_args/seq_block_ids.npy --input=@/data/llama-3.1/weights/405b/prefill_args/cache_state_f16.npy --benchmark_repetitions=3\n\nsharktank/sharktank/utils/export_artifacts.py:278: IreeBenchmarkException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/405b/fp16/llama3.1_405b_fp16.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_decomposed.json --bs=4 --attention-kernel decomposed\nINFO     eval:export_artifacts.py:186 Exported to mlir successfully:\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_decomposed.mlir&amp;#x27;\n\nINFO     eval:export_artifacts.py:117  export_to_mlir: 0:04:30\nINFO     eval:export_artifacts.py:117  compile_to_vmfb: 0:29:39\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_TP8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_TP8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;00:33:11&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_TP8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Compile Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:33:11&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_f16_TP8_Non_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(reason=&amp;quot;Compile Error&amp;quot;, strict=True, raises=IreeCompileException)\n    def testBenchmark405B_f16_TP8_Non_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;f16_torch&amp;quot;\n        output_mlir = self.llama405b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama405b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama405b_f16_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;fp16/tp8/llama3.1_405b_fp16_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n        export_return_code = self.llama405b_f16_torch_sdpa_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n&amp;gt;       self.llama405b_f16_torch_sdpa_artifacts.compile_to_vmfb(\n            mlir_path=str(output_mlir),\n            vmfb_path=output_vmfb,\n            hal_dump_path=output_file_name,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:725: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a4e8dd0&amp;gt;\n\n    @timeit\n    def compile_to_vmfb(\n        self,\n        *,\n        mlir_path,\n        vmfb_path,\n        cwd,\n        hal_dump_path: Optional[Path] = None,\n    ):\n        # TODO: Control flag to enable multiple backends\n        compile_args = [\n            f&amp;quot;iree-compile&amp;quot;,\n            f&amp;quot;{mlir_path}&amp;quot;,\n            f&amp;quot;--iree-hip-target={self.iree_hip_target}&amp;quot;,\n            f&amp;quot;--iree-hal-target-backends={self.iree_hal_target_backends}&amp;quot;,\n            f&amp;quot;-o={vmfb_path}&amp;quot;,\n        ]\n        if self.tensor_parallelism_size &amp;gt; 1:\n            iree_hal_target_devices = [\n                f&amp;quot;--iree-hal-target-device=hip[{i}]&amp;quot;\n                for i in range(self.tensor_parallelism_size)\n            ]\n            compile_args += iree_hal_target_devices\n        if hal_dump_path:\n            compile_args += [\n                f&amp;quot;--iree-hal-dump-executable-files-to={hal_dump_path}/files&amp;quot;\n            ]\n    \n        cmd = subprocess.list2cmdline(compile_args)\n    \n        logging.getLogger().info(f&amp;quot;Launching compile command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise IreeCompileException(proc, cwd)\nE           sharktank.utils.export_artifacts.IreeCompileException: Error invoking iree-compile\nE           Error code: 1\nE           Stderr diagnostics:\nE           failed to translate executables\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch/files/configured_module_decode_bs4$async_dispatch_29.mlir:2:2: error: failed to run translation of source executable to target executable for backend #hal.executable.target&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;\nE             hal.executable.variant public @rocm_hsaco_fb target(&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;) {\nE            ^\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch/files/configured_module_decode_bs4$async_dispatch_29.mlir:2:2: note: see current operation: \nE           &amp;quot;hal.executable.variant&amp;quot;() ({\nE             &amp;quot;hal.executable.export&amp;quot;() ({\nE             ^bb0(%arg1: !hal.device, %arg2: index, %arg3: index, %arg4: index):\nE               %37 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 16 : index}&amp;gt; : () -&amp;gt; index\nE               %38 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 8 : index}&amp;gt; : () -&amp;gt; index\nE               %39 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 4 : index}&amp;gt; : () -&amp;gt; index\nE               &amp;quot;hal.return&amp;quot;(%37, %38, %39) : (index, index, index) -&amp;gt; ()\nE             }) {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 0 : index, subgroup_size = 64 : index, sym_name = &amp;quot;decode_bs4$async_dispatch_29_attention_4xDx16x8x16x128xf16_dispatch_tensor_store&amp;quot;, workgroup_size = [128 : index, 1 : index, 1 : index]} : () -&amp;gt; ()\nE             &amp;quot;builtin.module&amp;quot;() ({\nE               &amp;quot;func.func&amp;quot;() &amp;lt;{function_type = () -&amp;gt; (), sym_name = &amp;quot;decode_bs4$async_dispatch_29_attention_4xDx16x8x16x128xf16_dispatch_tensor_store&amp;quot;}&amp;gt; ({\nE                 %0 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 0 : index}&amp;gt; : () -&amp;gt; index\nE                 %1 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 32 : i64}&amp;gt; : () -&amp;gt; i64\nE                 %2 = &amp;quot;arith.constant&amp;quot;() &amp;lt;{value = 8.837890e-02 : f16}&amp;gt; : () -&amp;gt; f16\nE                 %3 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 0 : index} : () -&amp;gt; i32\nE                 %4 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 1 : index} : () -&amp;gt; i32\nE                 %5 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 2 : index} : () -&amp;gt; i32\nE                 %6 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 3 : index} : () -&amp;gt; i32\nE                 %7 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 4 : index} : () -&amp;gt; i32\nE                 %8 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 5 : index} : () -&amp;gt; i32\nE                 %9 = &amp;quot;hal.interface.constant.load&amp;quot;() {layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, ordinal = 6 : index} : () -&amp;gt; i32\nE                 %10 = &amp;quot;arith.index_castui&amp;quot;(%3) : (i32) -&amp;gt; index\nE                 %11 = &amp;quot;arith.extui&amp;quot;(%4) : (i32) -&amp;gt; i64\nE                 %12 = &amp;quot;arith.extui&amp;quot;(%5) : (i32) -&amp;gt; i64\nE                 %13 = &amp;quot;arith.shli&amp;quot;(%12, %1) &amp;lt;{overflowFlags = #arith.overflow&amp;lt;none&amp;gt;}&amp;gt; : (i64, i64) -&amp;gt; i64\nE                 %14 = &amp;quot;arith.ori&amp;quot;(%11, %13) : (i64, i64) -&amp;gt; i64\nE                 %15 = &amp;quot;arith.index_castui&amp;quot;(%14) : (i64) -&amp;gt; index\nE                 %16 = &amp;quot;arith.extui&amp;quot;(%6) : (i32) -&amp;gt; i64\nE                 %17 = &amp;quot;arith.extui&amp;quot;(%7) : (i32) -&amp;gt; i64\nE                 %18 = &amp;quot;arith.shli&amp;quot;(%17, %1) &amp;lt;{overflowFlags = #arith.overflow&amp;lt;none&amp;gt;}&amp;gt; : (i64, i64) -&amp;gt; i64\nE                 %19 = &amp;quot;arith.ori&amp;quot;(%16, %18) : (i64, i64) -&amp;gt; i64\nE                 %20 = &amp;quot;arith.index_castui&amp;quot;(%19) : (i64) -&amp;gt; index\nE                 %21 = &amp;quot;arith.index_castui&amp;quot;(%8) : (i32) -&amp;gt; index\nE                 %22 = &amp;quot;arith.index_castui&amp;quot;(%9) : (i32) -&amp;gt; index\nE                 %23:5 = &amp;quot;util.assume.int&amp;quot;(%10, %15, %20, %21, %22) &amp;lt;{assumptions = [[#util.int.assumption&amp;lt;umin = 67504320, umax = 2215249920&amp;gt;], [#util.int.assumption&amp;lt;umin = 69601472, umax = 19393021952&amp;gt;], [#util.int.assumption&amp;lt;umin = 71698624, umax = 36570793984&amp;gt;], [#util.int.assumption&amp;lt;umin = 147904, umax = 147904, udiv = 147904&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;, #util.int.assumption&amp;lt;umin = 278976, umax = 278976, udiv = 278976&amp;gt;], [#util.int.assumption&amp;lt;umin = 1, umax = 8191&amp;gt;]]}&amp;gt; : (index, index, index, index, index) -&amp;gt; (index, index, index, index, index)\nE                 %24 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%0) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 0&amp;gt;} : (index) -&amp;gt; memref&amp;lt;4x8x16x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%24) &amp;lt;{alignment = 64 : i32}&amp;gt; : (memref&amp;lt;4x8x16x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %25 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#3) {alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 0&amp;gt;} : (index) -&amp;gt; memref&amp;lt;4x8x16x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%25) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x8x16x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %26 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 2 : index} : () -&amp;gt; index\nE                 %27 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 1 : index} : () -&amp;gt; index\nE                 %28 = &amp;quot;hal.interface.workgroup.id&amp;quot;() {dimension = 0 : index} : () -&amp;gt; index\nE                 %29 = &amp;quot;memref.subview&amp;quot;(%25, %26, %27, %28) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 0, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x16x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %30 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#0, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x?x16x8x16x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%30) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x?x16x8x16x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %31 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#1, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x?x16x8x16x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%31) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x?x16x8x16x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %32 = &amp;quot;hal.interface.binding.subspan&amp;quot;(%23#2, %23#4) {alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout&amp;lt;constants = 7, bindings = [#hal.pipeline.binding&amp;lt;storage_buffer, &amp;quot;ReadOnly|Indirect&amp;quot;&amp;gt;, #hal.pipeline.binding&amp;lt;storage_buffer, Indirect&amp;gt;], flags = Indirect&amp;gt;, operandSegmentSizes = array&amp;lt;i32: 1, 1&amp;gt;} : (index, index) -&amp;gt; memref&amp;lt;4x8x16x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;memref.assume_alignment&amp;quot;(%32) &amp;lt;{alignment = 1 : i32}&amp;gt; : (memref&amp;lt;4x8x16x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 %33 = &amp;quot;memref.subview&amp;quot;(%24, %26, %27, %28) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 0, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x16x1x128xf16, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %34 = &amp;quot;memref.subview&amp;quot;(%30, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, 0, 0, -9223372036854775808, -9223372036854775808, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, -9223372036854775808, 16, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x?x16x8x16x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %35 = &amp;quot;memref.subview&amp;quot;(%31, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, 0, 0, -9223372036854775808, -9223372036854775808, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, -9223372036854775808, 16, 1, 1, 128&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x?x16x8x16x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 %36 = &amp;quot;memref.subview&amp;quot;(%32, %26, %27, %28, %23#4) &amp;lt;{operandSegmentSizes = array&amp;lt;i32: 1, 3, 1, 0&amp;gt;, static_offsets = array&amp;lt;i64: -9223372036854775808, -9223372036854775808, -9223372036854775808, 0, 0, 0&amp;gt;, static_sizes = array&amp;lt;i64: 1, 1, 1, 1, -9223372036854775808, 16&amp;gt;, static_strides = array&amp;lt;i64: 1, 1, 1, 1, 1, 1&amp;gt;}&amp;gt; : (memref&amp;lt;4x8x16x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, index, index, index, index) -&amp;gt; memref&amp;lt;1x1x1x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;\nE                 &amp;quot;iree_linalg_ext.attention&amp;quot;(%33, %34, %35, %2, %36, %29) &amp;lt;{indexing_maps = [affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d5)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d6, d7, d1, d2, d5)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d6, d7, d1, d2, d4)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; ()&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d6, d7)&amp;gt;, affine_map&amp;lt;(d0, d1, d2, d3, d4, d5, d6, d7) -&amp;gt; (d0, d1, d2, d3, d4)&amp;gt;]}&amp;gt; ({\nE                 ^bb0(%arg0: f32):\nE                   &amp;quot;iree_linalg_ext.yield&amp;quot;(%arg0) : (f32) -&amp;gt; ()\nE                 }) : (memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x?x16x1x1x128xf16, strided&amp;lt;[?, 262144, 16384, 2048, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, f16, memref&amp;lt;1x1x1x1x?x16xf16, strided&amp;lt;[?, ?, ?, ?, 16, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;, memref&amp;lt;1x1x1x1x128xf16, strided&amp;lt;[16384, 2048, 128, 128, 1], offset: ?&amp;gt;, #gpu.address_space&amp;lt;global&amp;gt;&amp;gt;) -&amp;gt; ()\nE                 &amp;quot;func.return&amp;quot;() : () -&amp;gt; ()\nE               }) : () -&amp;gt; ()\nE             }) : () -&amp;gt; ()\nE             &amp;quot;hal.executable.variant_end&amp;quot;() : () -&amp;gt; ()\nE           }) {sym_name = &amp;quot;rocm_hsaco_fb&amp;quot;, target = #hal.executable.target&amp;lt;&amp;quot;rocm&amp;quot;, &amp;quot;rocm-hsaco-fb&amp;quot;, {abi = &amp;quot;hip&amp;quot;, iree.gpu.target = #iree_gpu.target&amp;lt;arch = &amp;quot;gfx942&amp;quot;, features = &amp;quot;&amp;quot;, wgp = &amp;lt;compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [&amp;lt;MFMA_F32_16x16x4_F32&amp;gt;, &amp;lt;MFMA_F32_16x16x16_F16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_F16&amp;gt;, &amp;lt;MFMA_F32_16x16x16_BF16&amp;gt;, &amp;lt;MFMA_F32_32x32x8_BF16&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E4M3FNUZ&amp;gt;, &amp;lt;MFMA_F32_16x16x32_F8E5M2FNUZ&amp;gt;, &amp;lt;MFMA_I32_16x16x32_I8&amp;gt;, &amp;lt;MFMA_I32_32x32x16_I8&amp;gt;], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384&amp;gt;&amp;gt;, ukernels = &amp;quot;none&amp;quot;}&amp;gt;} : () -&amp;gt; ()\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; iree-compile /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch.mlir --iree-hip-target=gfx942 --iree-hal-target-backends=rocm -o=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch.vmfb --iree-hal-target-device=hip[0] --iree-hal-target-device=hip[1] --iree-hal-target-device=hip[2] --iree-hal-target-device=hip[3] --iree-hal-target-device=hip[4] --iree-hal-target-device=hip[5] --iree-hal-target-device=hip[6] --iree-hal-target-device=hip[7] --iree-hal-dump-executable-files-to=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch/files\n\nsharktank/sharktank/utils/export_artifacts.py:224: IreeCompileException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/405b/fp16/llama3.1_405b_fp16.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch.json --bs=4 --attention-kernel torch\nINFO     eval:export_artifacts.py:186 Exported to mlir successfully:\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/f16_torch.mlir&amp;#x27;\n\nINFO     eval:export_artifacts.py:117  export_to_mlir: 0:03:42\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_TP8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_TP8_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_TP8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Test not yet implemented&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_fp8_TP8_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Test not yet implemented&amp;quot;, strict=True, raises=ExportMlirException\n    )\n    def testBenchmark405B_fp8_TP8_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;fp8_decomposed&amp;quot;\n        output_mlir = self.llama405b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama405b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama405b_fp8_decomposed_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;f8/tp8/llama3.1_405b_f8_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n&amp;gt;       export_return_code = self.llama405b_fp8_decomposed_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:768: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a4a1dd0&amp;gt;\n\n    @timeit\n    def export_to_mlir(\n        self,\n        *,\n        mlir_path: str,\n        json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            f&amp;quot;--irpa-file={self.irpa_path}&amp;quot;,\n            f&amp;quot;--output-mlir={mlir_path}&amp;quot;,\n            f&amp;quot;--output-config={json_path}&amp;quot;,\n            f&amp;quot;--bs={str(self.batch_size)}&amp;quot;,\n        ]\n        if self.attention_kernel in [&amp;quot;decomposed&amp;quot;, &amp;quot;torch&amp;quot;]:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(self.attention_kernel)\n    \n        cwd = self.sharktank_dir\n        cmd = subprocess.list2cmdline(export_args)\n    \n        logger.info(f&amp;quot;Exporting mlir:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n    \n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd, text=True)\n        if proc.returncode != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           sharktank.utils.export_artifacts.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 336, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 69, in main\nE               hp = configs.LlamaHParams.from_gguf_props(dataset.properties)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/configs/llm_configs.py&amp;quot;, line 47, in from_gguf_props\nE               name_prefix = p[&amp;quot;general.architecture&amp;quot;]\nE                             ~^^^^^^^^^^^^^^^^^^^^^^^^\nE           KeyError: &amp;#x27;general.architecture&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/405b/f8/llama405b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_decomposed.json --bs=4 --attention-kernel decomposed\n\nsharktank/sharktank/utils/export_artifacts.py:184: ExportMlirException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/405b/f8/llama405b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_decomposed.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_decomposed.json --bs=4 --attention-kernel decomposed\n\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_TP8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_TP8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_test.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_TP8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td&gt;Test not yet implemented&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_test.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_fp8_TP8_Non_Decomposed&amp;gt;\n\n    @pytest.mark.xfail(\n        reason=&amp;quot;Test not yet implemented&amp;quot;, strict=True, raises=ExportMlirException\n    )\n    def testBenchmark405B_fp8_TP8_Non_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;fp8_torch&amp;quot;\n        output_mlir = self.llama405b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name\n        )\n        output_json = self.llama405b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name\n        )\n        output_vmfb = self.llama405b_fp8_torch_sdpa_artifacts.create_file(\n            suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name\n        )\n        output_shard_file_name = (\n            self.artifacts_dir\n            / f&amp;quot;f8/tp8/llama3.1_405b_f8_tp{self.tensor_parallelism_size}_parameters.irpa&amp;quot;\n        )\n        if output_shard_file_name.exists():\n            self.irpa_path = output_shard_file_name\n&amp;gt;       export_return_code = self.llama405b_fp8_torch_sdpa_artifacts.export_to_mlir(\n            mlir_path=output_mlir,\n            json_path=output_json,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_test.py:815: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/sharktank/utils/export_artifacts.py:108: in wrapper\n    result = func(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;sharktank.utils.export_artifacts.ExportArtifacts object at 0x7dbd8a4ced50&amp;gt;\n\n    @timeit\n    def export_to_mlir(\n        self,\n        *,\n        mlir_path: str,\n        json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            f&amp;quot;--irpa-file={self.irpa_path}&amp;quot;,\n            f&amp;quot;--output-mlir={mlir_path}&amp;quot;,\n            f&amp;quot;--output-config={json_path}&amp;quot;,\n            f&amp;quot;--bs={str(self.batch_size)}&amp;quot;,\n        ]\n        if self.attention_kernel in [&amp;quot;decomposed&amp;quot;, &amp;quot;torch&amp;quot;]:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(self.attention_kernel)\n    \n        cwd = self.sharktank_dir\n        cmd = subprocess.list2cmdline(export_args)\n    \n        logger.info(f&amp;quot;Exporting mlir:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n    \n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd, text=True)\n        if proc.returncode != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           sharktank.utils.export_artifacts.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 336, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 69, in main\nE               hp = configs.LlamaHParams.from_gguf_props(dataset.properties)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/configs/llm_configs.py&amp;quot;, line 47, in from_gguf_props\nE               name_prefix = p[&amp;quot;general.architecture&amp;quot;]\nE                             ~^^^^^^^^^^^^^^^^^^^^^^^^\nE           KeyError: &amp;#x27;general.architecture&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/405b/f8/llama405b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_torch.json --bs=4 --attention-kernel torch\n\nsharktank/sharktank/utils/export_artifacts.py:184: ExportMlirException\n\n------------------------------ Captured log call -------------------------------\nINFO     eval:export_artifacts.py:180 Exporting mlir:\ncd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/405b/f8/llama405b_fp8.irpa --output-mlir=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_torch.mlir --output-config=/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-11-15/llama-405b/fp8_torch.json --bs=4 --attention-kernel torch\n\n&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;index.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
</html>