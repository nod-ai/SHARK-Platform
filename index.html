<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">index.html</title>
      <link href="assets/style.css" rel="stylesheet" type="text/css"/>
  </head>
  <body>
    <h1 id="title">index.html</h1>
    <p>Report generated on 24-Oct-2024 at 03:22:56 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="4">No results found. Check the filters.</th>
        </tr>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="4">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left"><</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">></div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">12 tests took 00:03:51.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" disabled/>
            <span class="failed">0 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" />
            <span class="passed">1 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" disabled/>
            <span class="skipped">0 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" />
            <span class="xfailed">11 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled/>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" disabled/>
            <span class="error">0 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" disabled/>
            <span class="rerun">0 Reruns</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  </body>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.11.10&#34;, &#34;Platform&#34;: &#34;Linux-6.8.0-47-generic-x86_64-with-glibc2.35&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;8.0.0&#34;, &#34;pluggy&#34;: &#34;1.5.0&#34;}, &#34;Plugins&#34;: {&#34;anyio&#34;: &#34;4.6.2.post1&#34;, &#34;html&#34;: &#34;4.1.1&#34;, &#34;xdist&#34;: &#34;3.5.0&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;}, &#34;CI&#34;: &#34;true&#34;}, &#34;tests&#34;: {&#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Decomposed&#34;, &#34;duration&#34;: &#34;00:01:52&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:01:52&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Non_Decomposed&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_f16_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_f16_Non_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;torch_sdpa not yet plumbed through&amp;quot;, strict=True)\n    def testBenchmark8B_f16_Non_Decomposed(self):\n        output_file_name = self.dir_path_8b / &amp;quot;f16_torch_sdpa&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;torch_sdpa&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:353: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:172: in export_mlir\n    cmd = self.get_export_cmd(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_f16_Non_Decomposed&amp;gt;\n\n    def get_export_cmd(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            &amp;quot;--irpa-file&amp;quot;,\n            irpa_path,\n            &amp;quot;--output-mlir&amp;quot;,\n            output_mlir_path,\n            &amp;quot;--output-config&amp;quot;,\n            output_json_path,\n        ]\n        if attention_kernel == &amp;quot;decomposed&amp;quot;:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(attention_kernel)\n        elif attention_kernel == &amp;quot;torch_sdpa&amp;quot;:\n&amp;gt;           raise NotImplementedError(\n                &amp;quot;attention_kernel torch_sdpa not yet plumbed through&amp;quot;\n            )\nE           NotImplementedError: attention_kernel torch_sdpa not yet plumbed through\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:137: NotImplementedError\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_fp8_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;8B fp8 irpa path not stored yet&amp;quot;, strict=True)\n    def testBenchmark8B_fp8_Decomposed(self):\n        output_file_name = self.dir_path_8b / &amp;quot;fp8_decomposed&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;decomposed&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path_fp8,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:395: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_fp8_Decomposed&amp;gt;\n\n    def export_mlir(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs export_paged_llm_v1.py and exports an MLIR file.\n        Args:\n            irpa_path: Path to the model irpa file.\n            output_mlir_path: Path to the file to save the exported file.\n            output_json_path: Path to the file to save the config json file.\n        &amp;quot;&amp;quot;&amp;quot;\n        cmd = self.get_export_cmd(\n            attention_kernel=attention_kernel,\n            tensor_parallelism_size=tensor_parallelism_size,\n            irpa_path=irpa_path,\n            output_mlir_path=output_mlir_path,\n            output_json_path=output_json_path,\n        )\n        logging.getLogger().info(f&amp;quot;Launching export command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           tests.models.llama.benchmark_amdgpu_tests.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 314, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 72, in main\nE               dataset = cli.get_input_dataset(args)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/utils/cli.py&amp;quot;, line 107, in get_input_dataset\nE               return Dataset.load(data_files[&amp;quot;irpa&amp;quot;], file_type=&amp;quot;irpa&amp;quot;)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 347, in load\nE               ds = _dataset_load_helper(path, file_type=file_type, mmap=mmap)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 538, in _dataset_load_helper\nE               return _dataset_load_irpa(path, mmap=mmap)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 548, in _dataset_load_irpa\nE               archive = ParameterArchive(path, mmap=mmap)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 223, in __init__\nE               self.load(file_path, mmap=mmap, readable=readable, writable=writable)\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 234, in load\nE               self._index.load(\nE           ValueError: Error opening parameter file: c/runtime/src/iree/base/internal/file_io.c:253: NOT_FOUND; failed to open file &amp;#x27;/data/llama-3.1/8b/llama8b_fp8.irpa&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file /data/llama-3.1/8b/llama8b_fp8.irpa --output-mlir /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-8b/fp8_decomposed.mlir --output-config /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-8b/fp8_decomposed.json --attention-kernel decomposed --tensor-parallelism-size 1\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:183: ExportMlirException\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_8B::testBenchmark8B_fp8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_fp8_Non_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;torch_sdpa not yet plumbed through&amp;quot;, strict=True)\n    def testBenchmark8B_fp8_Non_Decomposed(self):\n        output_file_name = self.dir_path_8b / &amp;quot;fp8_torch_sdpa&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;torch_sdpa&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path_fp8,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:437: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:172: in export_mlir\n    cmd = self.get_export_cmd(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_8B testMethod=testBenchmark8B_fp8_Non_Decomposed&amp;gt;\n\n    def get_export_cmd(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            &amp;quot;--irpa-file&amp;quot;,\n            irpa_path,\n            &amp;quot;--output-mlir&amp;quot;,\n            output_mlir_path,\n            &amp;quot;--output-config&amp;quot;,\n            output_json_path,\n        ]\n        if attention_kernel == &amp;quot;decomposed&amp;quot;:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(attention_kernel)\n        elif attention_kernel == &amp;quot;torch_sdpa&amp;quot;:\n&amp;gt;           raise NotImplementedError(\n                &amp;quot;attention_kernel torch_sdpa not yet plumbed through&amp;quot;\n            )\nE           NotImplementedError: attention_kernel torch_sdpa not yet plumbed through\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:137: NotImplementedError\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_f16_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;70b f16 irpa path not stored yet&amp;quot;, strict=True)\n    def testBenchmark70B_f16_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;f16_decomposed&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;decomposed&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:534: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_f16_Decomposed&amp;gt;\n\n    def export_mlir(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs export_paged_llm_v1.py and exports an MLIR file.\n        Args:\n            irpa_path: Path to the model irpa file.\n            output_mlir_path: Path to the file to save the exported file.\n            output_json_path: Path to the file to save the config json file.\n        &amp;quot;&amp;quot;&amp;quot;\n        cmd = self.get_export_cmd(\n            attention_kernel=attention_kernel,\n            tensor_parallelism_size=tensor_parallelism_size,\n            irpa_path=irpa_path,\n            output_mlir_path=output_mlir_path,\n            output_json_path=output_json_path,\n        )\n        logging.getLogger().info(f&amp;quot;Launching export command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           tests.models.llama.benchmark_amdgpu_tests.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 314, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 72, in main\nE               dataset = cli.get_input_dataset(args)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/utils/cli.py&amp;quot;, line 107, in get_input_dataset\nE               return Dataset.load(data_files[&amp;quot;irpa&amp;quot;], file_type=&amp;quot;irpa&amp;quot;)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 347, in load\nE               ds = _dataset_load_helper(path, file_type=file_type, mmap=mmap)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 538, in _dataset_load_helper\nE               return _dataset_load_irpa(path, mmap=mmap)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 548, in _dataset_load_irpa\nE               archive = ParameterArchive(path, mmap=mmap)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 223, in __init__\nE               self.load(file_path, mmap=mmap, readable=readable, writable=writable)\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 234, in load\nE               self._index.load(\nE           ValueError: Error opening parameter file: c/runtime/src/iree/base/internal/file_io.c:253: NOT_FOUND; failed to open file &amp;#x27;/data/llama-3.1/70b/llama70b_f16.irpa&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file /data/llama-3.1/70b/llama70b_f16.irpa --output-mlir /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-70b/f16_decomposed.mlir --output-config /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-70b/f16_decomposed.json --attention-kernel decomposed --tensor-parallelism-size 1\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:183: ExportMlirException\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_Non_Decomposed&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_f16_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_f16_Non_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;torch_sdpa not yet plumbed through&amp;quot;, strict=True)\n    def testBenchmark70B_f16_Non_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;f16_torch_sdpa&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;torch_sdpa&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:576: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:172: in export_mlir\n    cmd = self.get_export_cmd(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_f16_Non_Decomposed&amp;gt;\n\n    def get_export_cmd(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            &amp;quot;--irpa-file&amp;quot;,\n            irpa_path,\n            &amp;quot;--output-mlir&amp;quot;,\n            output_mlir_path,\n            &amp;quot;--output-config&amp;quot;,\n            output_json_path,\n        ]\n        if attention_kernel == &amp;quot;decomposed&amp;quot;:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(attention_kernel)\n        elif attention_kernel == &amp;quot;torch_sdpa&amp;quot;:\n&amp;gt;           raise NotImplementedError(\n                &amp;quot;attention_kernel torch_sdpa not yet plumbed through&amp;quot;\n            )\nE           NotImplementedError: attention_kernel torch_sdpa not yet plumbed through\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:137: NotImplementedError\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_fp8_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;70B fp8 irpa path not stored yet&amp;quot;, strict=True)\n    def testBenchmark70B_fp8_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;fp8_decomposed&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;decomposed&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path_fp8,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:618: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_fp8_Decomposed&amp;gt;\n\n    def export_mlir(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs export_paged_llm_v1.py and exports an MLIR file.\n        Args:\n            irpa_path: Path to the model irpa file.\n            output_mlir_path: Path to the file to save the exported file.\n            output_json_path: Path to the file to save the config json file.\n        &amp;quot;&amp;quot;&amp;quot;\n        cmd = self.get_export_cmd(\n            attention_kernel=attention_kernel,\n            tensor_parallelism_size=tensor_parallelism_size,\n            irpa_path=irpa_path,\n            output_mlir_path=output_mlir_path,\n            output_json_path=output_json_path,\n        )\n        logging.getLogger().info(f&amp;quot;Launching export command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           tests.models.llama.benchmark_amdgpu_tests.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 314, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 72, in main\nE               dataset = cli.get_input_dataset(args)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/utils/cli.py&amp;quot;, line 107, in get_input_dataset\nE               return Dataset.load(data_files[&amp;quot;irpa&amp;quot;], file_type=&amp;quot;irpa&amp;quot;)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 347, in load\nE               ds = _dataset_load_helper(path, file_type=file_type, mmap=mmap)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 538, in _dataset_load_helper\nE               return _dataset_load_irpa(path, mmap=mmap)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 548, in _dataset_load_irpa\nE               archive = ParameterArchive(path, mmap=mmap)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 223, in __init__\nE               self.load(file_path, mmap=mmap, readable=readable, writable=writable)\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 234, in load\nE               self._index.load(\nE           ValueError: Error opening parameter file: c/runtime/src/iree/base/internal/file_io.c:253: NOT_FOUND; failed to open file &amp;#x27;/data/llama-3.1/70b/llama70b_fp8.irpa&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file /data/llama-3.1/70b/llama70b_fp8.irpa --output-mlir /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-70b/fp8_decomposed.mlir --output-config /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-70b/fp8_decomposed.json --attention-kernel decomposed --tensor-parallelism-size 1\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:183: ExportMlirException\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_70B::testBenchmark70B_fp8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_fp8_Non_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;torch_sdpa not yet plumbed through&amp;quot;, strict=True)\n    def testBenchmark70B_fp8_Non_Decomposed(self):\n        output_file_name = self.dir_path_70b / &amp;quot;fp8_torch_sdpa&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;torch_sdpa&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path_fp8,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:660: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:172: in export_mlir\n    cmd = self.get_export_cmd(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_70B testMethod=testBenchmark70B_fp8_Non_Decomposed&amp;gt;\n\n    def get_export_cmd(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            &amp;quot;--irpa-file&amp;quot;,\n            irpa_path,\n            &amp;quot;--output-mlir&amp;quot;,\n            output_mlir_path,\n            &amp;quot;--output-config&amp;quot;,\n            output_json_path,\n        ]\n        if attention_kernel == &amp;quot;decomposed&amp;quot;:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(attention_kernel)\n        elif attention_kernel == &amp;quot;torch_sdpa&amp;quot;:\n&amp;gt;           raise NotImplementedError(\n                &amp;quot;attention_kernel torch_sdpa not yet plumbed through&amp;quot;\n            )\nE           NotImplementedError: attention_kernel torch_sdpa not yet plumbed through\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:137: NotImplementedError\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_Decomposed&#34;, &#34;duration&#34;: &#34;00:01:52&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:01:52&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_f16_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;405B f16 irpa path not stored yet&amp;quot;, strict=True)\n    def testBenchmark405B_f16_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;f16_decomposed&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;decomposed&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:757: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_f16_Decomposed&amp;gt;\n\n    def export_mlir(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs export_paged_llm_v1.py and exports an MLIR file.\n        Args:\n            irpa_path: Path to the model irpa file.\n            output_mlir_path: Path to the file to save the exported file.\n            output_json_path: Path to the file to save the config json file.\n        &amp;quot;&amp;quot;&amp;quot;\n        cmd = self.get_export_cmd(\n            attention_kernel=attention_kernel,\n            tensor_parallelism_size=tensor_parallelism_size,\n            irpa_path=irpa_path,\n            output_mlir_path=output_mlir_path,\n            output_json_path=output_json_path,\n        )\n        logging.getLogger().info(f&amp;quot;Launching export command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           tests.models.llama.benchmark_amdgpu_tests.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py:163: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\nE             return torch.from_numpy(wrapper)\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 314, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 296, in main\nE               generate_batch_prefill(bs)\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 181, in generate_batch_prefill\nE               @fxb.export_program(\nE                ^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/fx_programs.py&amp;quot;, line 239, in export_program\nE               program = torch.export.export(\nE                         ^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/__init__.py&amp;quot;, line 174, in export\nE               return _export(\nE                      ^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/_trace.py&amp;quot;, line 635, in wrapper\nE               raise e\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/_trace.py&amp;quot;, line 618, in wrapper\nE               ep = fn(*args, **kwargs)\nE                    ^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/exported_program.py&amp;quot;, line 83, in wrapper\nE               return fn(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/_trace.py&amp;quot;, line 788, in _export\nE               ep_non_strict = _export_non_strict(\nE                               ^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/_trace.py&amp;quot;, line 439, in _export_non_strict\nE               gm, graph_signature = transform(aot_export_module)(\nE                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/_trace.py&amp;quot;, line 749, in _aot_export_non_strict\nE               gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py&amp;quot;, line 1047, in aot_export_module\nE               fx_g, metadata, in_spec, out_spec = _aot_export_function(\nE                                                   ^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py&amp;quot;, line 1237, in _aot_export_function\nE               fx_g, meta = create_aot_dispatcher_function(\nE                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_dynamo/utils.py&amp;quot;, line 262, in time_wrapper\nE               r = func(*args, **kwargs)\nE                   ^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py&amp;quot;, line 533, in create_aot_dispatcher_function\nE               fw_metadata = run_functionalized_fw_and_collect_metadata(\nE                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py&amp;quot;, line 112, in inner\nE               flat_f_outs = f(*flat_f_args)\nE                             ^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&amp;quot;, line 171, in flat_fn\nE               tree_out = fn(*args, **kwargs)\nE                          ^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&amp;quot;, line 680, in functional_call\nE               out = mod(*args[params_len:], **kwargs)\nE                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/nn/modules/module.py&amp;quot;, line 1532, in _wrapped_call_impl\nE               return self._call_impl(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/nn/modules/module.py&amp;quot;, line 1541, in _call_impl\nE               return forward_call(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/export/_trace.py&amp;quot;, line 736, in forward\nE               tree_out = self._export_root(*args, **kwargs)\nE                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/nn/modules/module.py&amp;quot;, line 1532, in _wrapped_call_impl\nE               return self._call_impl(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/nn/modules/module.py&amp;quot;, line 1541, in _call_impl\nE               return forward_call(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/fx_programs.py&amp;quot;, line 226, in new_forward\nE               return f(self.root, *forward_args, **forward_kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 204, in _\nE               logits = model.prefill(\nE                        ^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/models/llama/llama.py&amp;quot;, line 137, in prefill\nE               h = self.token_embedding(tokens)\nE                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/nn/modules/module.py&amp;quot;, line 1532, in _wrapped_call_impl\nE               return self._call_impl(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/_tool/Python/3.11.10/x64/lib/python3.11/site-packages/torch/nn/modules/module.py&amp;quot;, line 1541, in _call_impl\nE               return forward_call(*args, **kwargs)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/layers/token_embedding.py&amp;quot;, line 26, in forward\nE               return ops.embedding_lookup(input, self.weight, dtype=self.dtype)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/ops/_registry.py&amp;quot;, line 199, in __call__\nE               selected_override, *results = trampoline(self, *args, **kwargs)\nE                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/ops/signatures.py&amp;quot;, line 248, in _embedding_lookup_trampoline\nE               d.fail(tensors)\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/ops/_registry.py&amp;quot;, line 248, in fail\nE               raise NotImplementedError(\nE           NotImplementedError: Overridable operator sharktank.ops.signatures.embedding_lookup does not have an implementation for argument types: [&amp;lt;class &amp;#x27;torch._subclasses.functional_tensor.FunctionalTensor&amp;#x27;&amp;gt;, &amp;lt;class &amp;#x27;sharktank.types.tensors.ReplicatedTensor&amp;#x27;&amp;gt;]\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file /data/llama-3.1/405b/llama405b_f16.irpa --output-mlir /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-405b/f16_decomposed.mlir --output-config /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-405b/f16_decomposed.json --attention-kernel decomposed --tensor-parallelism-size 8\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:183: ExportMlirException\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_Non_Decomposed&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_f16_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_f16_Non_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;torch_sdpa not yet plumbed through&amp;quot;, strict=True)\n    def testBenchmark405B_f16_Non_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;f16_torch_sdpa&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;torch_sdpa&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:799: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:172: in export_mlir\n    cmd = self.get_export_cmd(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_f16_Non_Decomposed&amp;gt;\n\n    def get_export_cmd(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            &amp;quot;--irpa-file&amp;quot;,\n            irpa_path,\n            &amp;quot;--output-mlir&amp;quot;,\n            output_mlir_path,\n            &amp;quot;--output-config&amp;quot;,\n            output_json_path,\n        ]\n        if attention_kernel == &amp;quot;decomposed&amp;quot;:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(attention_kernel)\n        elif attention_kernel == &amp;quot;torch_sdpa&amp;quot;:\n&amp;gt;           raise NotImplementedError(\n                &amp;quot;attention_kernel torch_sdpa not yet plumbed through&amp;quot;\n            )\nE           NotImplementedError: attention_kernel torch_sdpa not yet plumbed through\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:137: NotImplementedError\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_Decomposed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_fp8_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;405B fp8 irpa path not stored yet&amp;quot;, strict=True)\n    def testBenchmark405B_fp8_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;fp8_decomposed&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;decomposed&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path_fp8,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:841: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_fp8_Decomposed&amp;gt;\n\n    def export_mlir(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n        cwd: str | Path,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;Runs export_paged_llm_v1.py and exports an MLIR file.\n        Args:\n            irpa_path: Path to the model irpa file.\n            output_mlir_path: Path to the file to save the exported file.\n            output_json_path: Path to the file to save the config json file.\n        &amp;quot;&amp;quot;&amp;quot;\n        cmd = self.get_export_cmd(\n            attention_kernel=attention_kernel,\n            tensor_parallelism_size=tensor_parallelism_size,\n            irpa_path=irpa_path,\n            output_mlir_path=output_mlir_path,\n            output_json_path=output_json_path,\n        )\n        logging.getLogger().info(f&amp;quot;Launching export command:\\n&amp;quot; f&amp;quot;cd {cwd} &amp;amp;&amp;amp; {cmd}&amp;quot;)\n        proc = subprocess.run(cmd, shell=True, capture_output=True, cwd=cwd)\n        return_code = proc.returncode\n        if return_code != 0:\n&amp;gt;           raise ExportMlirException(proc, cwd)\nE           tests.models.llama.benchmark_amdgpu_tests.ExportMlirException: Error invoking export_paged_llama_v1.py\nE           Error code: 1\nE           Stderr diagnostics:\nE           Traceback (most recent call last):\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nE             File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 314, in &amp;lt;module&amp;gt;\nE               main()\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/examples/export_paged_llm_v1.py&amp;quot;, line 72, in main\nE               dataset = cli.get_input_dataset(args)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/utils/cli.py&amp;quot;, line 107, in get_input_dataset\nE               return Dataset.load(data_files[&amp;quot;irpa&amp;quot;], file_type=&amp;quot;irpa&amp;quot;)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 347, in load\nE               ds = _dataset_load_helper(path, file_type=file_type, mmap=mmap)\nE                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 538, in _dataset_load_helper\nE               return _dataset_load_irpa(path, mmap=mmap)\nE                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/sharktank/sharktank/types/theta.py&amp;quot;, line 548, in _dataset_load_irpa\nE               archive = ParameterArchive(path, mmap=mmap)\nE                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 223, in __init__\nE               self.load(file_path, mmap=mmap, readable=readable, writable=writable)\nE             File &amp;quot;/home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/deps/iree-turbine/iree/turbine/aot/params.py&amp;quot;, line 234, in load\nE               self._index.load(\nE           ValueError: Error opening parameter file: c/runtime/src/iree/base/internal/file_io.c:253: NOT_FOUND; failed to open file &amp;#x27;/data/llama-3.1/405b/llama405b_fp8.irpa&amp;#x27;\nE           \nE           \nE           Invoked with:\nE             cd /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform &amp;amp;&amp;amp; python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file /data/llama-3.1/405b/llama405b_fp8.irpa --output-mlir /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-405b/fp8_decomposed.mlir --output-config /home/sai/actions-runner-llama/_work/SHARK-Platform/SHARK-Platform/2024-10-24/llama-405b/fp8_decomposed.json --attention-kernel decomposed --tensor-parallelism-size 8\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:183: ExportMlirException\n&#34;}], &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_Non_Decomposed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;XFailed&#34;, &#34;testId&#34;: &#34;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_Non_Decomposed&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;XFailed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/models/llama/benchmark_amdgpu_tests.py::BenchmarkLlama3_1_405B::testBenchmark405B_fp8_Non_Decomposed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_fp8_Non_Decomposed&amp;gt;\n\n    @longrun\n    @is_mi300x\n    @pytest.mark.xfail(reason=&amp;quot;torch_sdpa not yet plumbed through&amp;quot;, strict=True)\n    def testBenchmark405B_fp8_Non_Decomposed(self):\n        output_file_name = self.dir_path_405b / &amp;quot;fp8_torch_sdpa&amp;quot;\n        output_mlir = self.create_file(suffix=&amp;quot;.mlir&amp;quot;, prefix=output_file_name)\n        output_json = self.create_file(suffix=&amp;quot;.json&amp;quot;, prefix=output_file_name)\n        output_vmfb = self.create_file(suffix=&amp;quot;.vmfb&amp;quot;, prefix=output_file_name)\n&amp;gt;       self.export_mlir(\n            attention_kernel=&amp;quot;torch_sdpa&amp;quot;,\n            tensor_parallelism_size=self.tensor_parallelism_size,\n            irpa_path=self.irpa_path_fp8,\n            output_mlir_path=output_mlir,\n            output_json_path=output_json,\n            cwd=self.repo_root,\n        )\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:883: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:172: in export_mlir\n    cmd = self.get_export_cmd(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;tests.models.llama.benchmark_amdgpu_tests.BenchmarkLlama3_1_405B testMethod=testBenchmark405B_fp8_Non_Decomposed&amp;gt;\n\n    def get_export_cmd(\n        self,\n        *,\n        attention_kernel: str,\n        tensor_parallelism_size: int,\n        irpa_path: str,\n        output_mlir_path: str,\n        output_json_path: str,\n    ):\n        export_args = [\n            &amp;quot;python3&amp;quot;,\n            &amp;quot;-m&amp;quot;,\n            &amp;quot;sharktank.examples.export_paged_llm_v1&amp;quot;,\n            &amp;quot;--irpa-file&amp;quot;,\n            irpa_path,\n            &amp;quot;--output-mlir&amp;quot;,\n            output_mlir_path,\n            &amp;quot;--output-config&amp;quot;,\n            output_json_path,\n        ]\n        if attention_kernel == &amp;quot;decomposed&amp;quot;:\n            export_args.append(&amp;quot;--attention-kernel&amp;quot;)\n            export_args.append(attention_kernel)\n        elif attention_kernel == &amp;quot;torch_sdpa&amp;quot;:\n&amp;gt;           raise NotImplementedError(\n                &amp;quot;attention_kernel torch_sdpa not yet plumbed through&amp;quot;\n            )\nE           NotImplementedError: attention_kernel torch_sdpa not yet plumbed through\n\nsharktank/tests/models/llama/benchmark_amdgpu_tests.py:137: NotImplementedError\n&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;index.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
</html>
